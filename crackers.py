#-*- coding: utf-8 -*-
import hashlib
import json
import operator
import time
import itertools

import celery
import redis

import decoder
import config
import heuristics
import utils


# object of cluster
app = celery.Celery()
# load cluster configuration from config module
app.config_from_object(config)


def get_functions(data):
    """
    Load succesful function from previous analyses and construct them into the chain
    This functions is used for automated analysis
    """
    r = redis.StrictRedis.from_url(config.redis_server)
    # 
    keys = r.keys("result-*")
    existing = []
    for key in keys:
        results = r.lrange(key, 0, -1)
        for result in results:  # Enumerate succesful results
            rdata = json.loads(result)
            operations = rdata["operations"]
            if operations in existing:
                continue  # Skip operations that are already in the list to avoid duplications
            elif len(operations) == 1 and operations[0][0] == "xor" and operations[0][1] > 0 and operations[0][2] == 0:
                continue  # Skip XOR only operations because they are added sepparately
            existing.append(operations)
            function = operations[0]
            chain = [decoder.decoders[x[0]] for x in operations[1:]]
            # Create a configuration for the selected operations
            cfg = [{"start": x[1], "stop": x[1], "key_change": x[2]} for x in rdata["operations"][1:]]
            # yield a reconstructed chain with configuration from loaded operations
            yield decoder.decoders[function[0]](data, start=function[1], stop=function[1], key_change=function[2], chain=chain, config=cfg)


def analyze(result, operations, session_key, result_key, score, md5, r):
    """
    Enumerate each result from operations and run rules on top of it
    """
    try:
        # Increment task progress
        r.hincrby(session_key, "current", 1)
        # Run Yara rules on the result
        rank, matches = score.run(result)
        if rank == 0:  # Yara rules didn't find anything, skip processing
            return
        else:
            for match in matches:
                for string in match["strings"]:
                    try:
                        string["data"].decode()  # Decode matched string by Yara
                    except UnicodeDecodeError:
                        # Matched string is a sequence of bytes, convert it to the HEX format
                        string["data"] = string["data"].encode("hex").upper()
            # store result as a file in the database by it's MD5 checksum
            md5_decoded = utils.store_file(result)
            # Metadata for the result
            meta = {
                "md5": md5_decoded,
                "matches": matches,
                "analyzed": time.time(),
                "rank": rank,
                "original": md5
            }
            r.set("decoded-"+md5_decoded, json.dumps(meta))
            r.expire("decoded-"+md5_decoded, config.expiration)
            # Store the information about the operations that has found the result
            r.rpush(result_key, json.dumps({"operations": operations, "rank": rank, "operations_str": utils.format_operations(operations), "md5": md5_decoded}))
    except UnicodeDecodeError:
        pass


@app.task(bind=True)
def brute_force(self, data, crack_functions, score_params):
    """
    Perform a brute force on submitted data
    data: loaded file as string
    crack_functions: a configuration of crack functions form the API frontend
    """
    r = redis.StrictRedis.from_url(config.redis_server)
    score = heuristics.Heuristics(*score_params)
    result_key = "result-" + self.request.id
    md5 = hashlib.md5(data).hexdigest()  # compute md5 of original data
    # Calculate a total number of brute force operations, used to display a progress
    total = reduce(operator.mul, [(f["config"][1]-f["config"][0]+1) for f in crack_functions], 1)
    r.hmset(self.request.id, {"current": 0, "total":total})
    # Convert functions by they name to the actual function object
    functions = [decoder.decoders[f["name"]] for f in crack_functions]
    # Create a list of optional configurations for the functions from submitted API data
    configs = [dict(zip(["start", "stop"],f["config"]), key_change=int(f.get("key_change", 0))) for f in crack_functions]
    for result, operations in functions[0](data, chain=functions[1:], config=configs[1:], **configs[0]):
        # Enumerate each set of operations and perform an analysis on the result
        analyze(result, operations, self.request.id, result_key, score, md5, r)
    r.expire(result_key, config.expiration)
    r.expire(self.request.id, config.expiration)


@app.task(bind=True)
def automated_analysis(self, data, score_params):
    """
    Create an automated analysis task for the submitted data by API
    """
    r = redis.StrictRedis.from_url(config.redis_server)
    score = heuristics.Heuristics(*score_params)
    result_key = "result-" + self.request.id
    md5 = hashlib.md5(data).hexdigest()  # compute md5 of original data
    # Add default XOR operations with keys 1-255
    functions = [decoder.decoders["xor"](data)]
    # Extend the list of operations by loading previous succesful analyses
    functions.extend(get_functions(data))
    total = 255 + len(functions[1:])  # Total number of operations for progress tracking
    r.hmset(self.request.id, {"current": 0, "total": total})
    for result, operations in itertools.chain(*functions):
        # Enumerate each set of operations and perform the analysis on the result
        analyze(result, operations, self.request.id, result_key, score, md5, r)
    r.expire(result_key, config.expiration)
    r.expire(self.request.id, config.expiration)
